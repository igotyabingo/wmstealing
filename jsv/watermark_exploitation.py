import argparse
from typing import List
from transformers import AutoTokenizer, AutoModelForCausalLM, T5ForConditionalGeneration, LogitsProcessorList
import torch
import tqdm
import json
import torch.nn.functional as F

from processors import SpoofedProcessor
from count_store import CountStore
from generation_dataset import GenerationPrompts
from extended_watermark_processor import WatermarkDetector

import nltk
from nltk.tokenize import sent_tokenize

def load_queries_and_learn(tokenizer: AutoTokenizer, texts_wm: List[str], dest_counts: CountStore, prevctx_width: int):
    # load queries then update count_store with base or wm (learn)
    for text in texts_wm:
        toks = tokenizer(text)["input_ids"]  
        for i in range(prevctx_width, len(toks)):
            ctx = tuple(toks[i - prevctx_width : i])
            dest_counts.add(ctx, toks[i], 1)

def generate(surrogate_model, surrogate_tokenizer, raw_prompt, prompt_ids, spoofed_processor: SpoofedProcessor):    
    
    attention_mask = torch.ones_like(prompt_ids).to(surrogate_model.device)
    generation_kwargs = dict(
        max_new_tokens=300, 
        do_sample=True,                
        pad_token_id=surrogate_tokenizer.eos_token_id, 
        attention_mask=attention_mask,
        logits_processor=LogitsProcessorList([spoofed_processor])
    )

    output_ids = surrogate_model.generate(
        prompt_ids.to(surrogate_model.device),
        **generation_kwargs
    )

    result_dict = {}
    result_dict["prompt"] = surrogate_tokenizer.decode(prompt_ids[0])
    result_dict["raw_prompt"] = raw_prompt
    result_dict["generated_text"] = surrogate_tokenizer.decode(output_ids[0][len(prompt_ids[0]):])

    return result_dict
        

def run_eval(surrogate_model, surrogate_tokenizer, target_gen_dataset, save_dir, spoofed_processor, detector):
    
    with torch.no_grad(): 
        for idx in range(len(target_gen_dataset)):
            print("running idx: ", idx)
            
            if idx%3 == 0:
                torch.cuda.empty_cache()

            prompt_ids, raw_prompt, prompt_idx = target_gen_dataset[idx]
            result_dict = generate(surrogate_model, surrogate_tokenizer, raw_prompt, prompt_ids, spoofed_processor=spoofed_processor) 
            p_value = detector.detect(result_dict["generated_text"]) 
            
            result_dict["p_value"] = p_value
            result_dict["prompt_idx"] = idx

            with open(save_dir, "a",) as f:
                f.write(json.dumps(result_dict) + "\n")

def main(args):
    torch.cuda.empty_cache()
    import gc
    gc.collect()

    counts_base = CountStore(args.prevctx_width)
    counts_wm = CountStore(args.prevctx_width)
    
    print("surrogate model loading\n")

    surrogate_tokenizer = AutoTokenizer.from_pretrained(args.surrogate_model_id)
    surrogate_model = AutoModelForCausalLM.from_pretrained(
        args.surrogate_model_id,
        device_map="auto",
        offload_folder="/scratch/cdltlehf/offload",
        torch_dtype="auto")
    surrogate_model.eval()

    print("surrogate model loaded")

    print("CountStore updating 1")

    base_text = []    
    # generate_output.py에서 생성한 output이 저장된 파일에서 generated text 부분만 불러온다.
    with open(f"./result/target_generation_jsv/{args.target_model_id}_base_c4_realnews.jsonl", "r") as f:
        for line in f:  
            base_text.append(json.loads(line)["generated_text"]) 
    load_queries_and_learn(surrogate_tokenizer, base_text, counts_base, args.prevctx_width)
    
    print("CountStore updating 2")
    wm_text = []
    with open(f"./result/target_generation_jsv/{args.target_model_id}_wm_c4_realnews.jsonl", "r") as f:
        for line in f:  
            wm_text.append(json.loads(line)["generated_text"])
    load_queries_and_learn(surrogate_tokenizer, wm_text, counts_wm, args.prevctx_width)
    # CountStore updated

    prompt_set_name = args.prompt_set_name # mww or dolly

    target_gen_dataset = GenerationPrompts( 
        model_id=args.surrogate_model_id,
        prompt_set_name=prompt_set_name,
        tokenizer=surrogate_tokenizer,
    )

    save_dir = f"./result/spoofing/jsv/{prompt_set_name}/{args.target_model_id}_wm_{prompt_set_name}.jsonl"

    spoofed_processor = SpoofedProcessor(counts_base=counts_base, counts_wm=counts_wm, prevctx_width=args.prevctx_width, vocab_size=surrogate_tokenizer.get_vocab(), tokenizer=surrogate_tokenizer)
    target_tokenizer = AutoTokenizer.from_pretrained(args.target_model_id)
    detector = WatermarkDetector(vocab=target_tokenizer.get_vocab(), seeding_scheme='selfhash', gamma=0.25, device='cuda', tokenizer=target_tokenizer, normalizers=[], z_threshold=4.0, ignore_repeated_ngrams=True) 

    run_eval(surrogate_model, surrogate_tokenizer, target_gen_dataset, save_dir, spoofed_processor, detector)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--target_model_id", type=str, default="meta-llama/Llama-3.2-3B-Instruct" 
    )
    parser.add_argument(
        "--surrogate_model_id", type=str, default="meta-llama/Llama-3.2-1B-Instruct" 
    )
    parser.add_argument("--prompt_set_name", type=str, default="mmw_book_report") 

    parser.add_argument("--prevctx_width", type=int, default=3)

    args = parser.parse_args()

    main(args)
